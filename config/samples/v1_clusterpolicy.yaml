apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  operator:
    defaultRuntime: crio
    runtimeClass: nvidia
    initContainer: {}
    use_ocp_driver_toolkit: false

  psp:
    enabled: false

  sandboxWorkloads:
    enabled: false
    defaultWorkload: "container"

  mig:
    strategy: single

  daemonsets: {}

  validator:
    plugin:
    env:
      - name: WITH_WORKLOAD
        value: "true"

  devicePlugin:
    config:
      name: ""
      default: ""

  gfd: {}

  toolkit:
    enabled: true

  dcgm:
    enabled: true

  dcgmExporter:
    # configmap name for custom dcgm metrics
    config:
      name: ""

  migManager:
    enabled: true
    config:
      name: ""
    gpuClientsConfig:
      name: ""

  nodeStatusExporter:
    enabled: false

  driver:
    enabled: true

    rdma:
      enabled: false
    # private mirror repository configuration
    repoConfig:
      configMapName: ""
    # custom ssl key/certificate configuration
    certConfig:
      name: ""
    # vGPU licensing configuration
    licensingConfig:
      configMapName: ""
      nlsEnabled: false
    # vGPU topology daemon configuration
    virtualTopology:
      config: ""
    # kernel module configuration for NVIDIA driver
    kernelModuleConfig:
      name: ""
    # configuration for controlling rolling update of NVIDIA driver DaemonSet pods
    rollingUpdate:
      # maximum number of nodes to simultaneously apply pod updates on.
      # can be specified either as number or percentage of nodes. Default 1.
      maxUnavailable: "1"

  vgpuManager:
    enabled: true

  vgpuDeviceManager:
    enabled: true
# NOTE: When sandboxedEnvironments is enabled, creating
# a custom ConfigMap with vGPU device configuration is required
# for a successful vGPU Device Manager deployent. Replace
# the below with the correct ConfigMap and default config
# in ConfigMap.
    config:
      name: "name-of-configmap"
      default: "name-of-default-config"

  vfioManager:
    enabled: true

  sandboxDevicePlugin: {}
